{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSadimba/milvusdb-test/blob/main/ingestion_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PL2Z6fCkTL5B",
        "outputId": "0d6953d6-ffad-401f-dabc-1fe305da7e5f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "Requirement already satisfied: pymilvus==2.4.4 in /usr/local/lib/python3.12/dist-packages (2.4.4)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.2)\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.12/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: setuptools>=67 in /usr/local/lib/python3.12/dist-packages (from pymilvus==2.4.4) (75.2.0)\n",
            "Requirement already satisfied: grpcio<=1.63.0,>=1.49.1 in /usr/local/lib/python3.12/dist-packages (from pymilvus==2.4.4) (1.63.0)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.12/dist-packages (from pymilvus==2.4.4) (5.29.5)\n",
            "Requirement already satisfied: environs<=9.5.0 in /usr/local/lib/python3.12/dist-packages (from pymilvus==2.4.4) (9.5.0)\n",
            "Requirement already satisfied: ujson>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pymilvus==2.4.4) (5.11.0)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.12/dist-packages (from pymilvus==2.4.4) (2.2.2)\n",
            "Requirement already satisfied: milvus-lite<2.5.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from pymilvus==2.4.4) (2.4.12)\n",
            "Requirement already satisfied: marshmallow>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from environs<=9.5.0->pymilvus==2.4.4) (3.26.1)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.12/dist-packages (from arxiv) (6.0.12)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.12/dist-packages (from arxiv) (2.32.5)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2025.11.12)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow>=3.0.0->environs<=9.5.0->pymilvus==2.4.4) (25.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus==2.4.4) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus==2.4.4) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus==2.4.4) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.4->pymilvus==2.4.4) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus==2.4.4) (1.17.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.3)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (0.4.56)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (0.12.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.15)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "\n",
        "# Python 3.12â€“compatible packages\n",
        "!pip install pymilvus==2.4.4 pypdf arxiv tqdm python-dotenv\n",
        "\n",
        "# LangChain + HuggingFace embeddings (safe)\n",
        "!pip install langchain langchain-community langchain-huggingface\n",
        "\n",
        "# PyTorch CPU (Python 3.12 compatible)\n",
        "!pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "# Transformers (safe for Python 3.12)\n",
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v18kswPcqHfa",
        "outputId": "67372828-3e9f-412b-f7cc-1cd85a47ab83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment OK\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "    from transformers import pipeline\n",
        "    from pypdf import PdfReader\n",
        "    import arxiv\n",
        "    from pymilvus import connections\n",
        "    print(\"Environment OK\")\n",
        "except Exception as e:\n",
        "    print(\"ERROR:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf6PtzaNqeyc",
        "outputId": "3ff8e927-af7f-4d33-f0c0-b24c01af7dc4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-cpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-intel as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-macos as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping keras as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping keras-nightly as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping keras-preprocessing as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y tensorflow tensorflow-cpu tensorflow-intel tensorflow-macos keras keras-nightly keras-preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUpHPZqCqhBk",
        "outputId": "de115a64-1184-462e-d07f-bbb22a0bd51e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers --no-deps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ntm8Nxw7r2Qe",
        "outputId": "313c8412-4a01-4fc5-d50f-f111d6d65561",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch --index-url https://download.pytorch.org/whl/cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5Xxth6ZtCcp",
        "outputId": "51d9843a-da07-43b0-eab0-71b713711e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment OK\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from transformers import pipeline\n",
        "    from langchain_huggingface import HuggingFaceEmbeddings\n",
        "    from pypdf import PdfReader\n",
        "    import arxiv\n",
        "    print(\"Environment OK\")\n",
        "except Exception as e:\n",
        "    print(\"ERROR:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfEq_OA6tGL0",
        "outputId": "32976864-9969-4f3d-f273-62dfd62f92b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config loaded.\n"
          ]
        }
      ],
      "source": [
        "import os, io, re, uuid, logging, textwrap\n",
        "from typing import List, Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "import requests\n",
        "import arxiv\n",
        "from pypdf import PdfReader\n",
        "\n",
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "from pymilvus import (\n",
        "    connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
        ")\n",
        "\n",
        "# ====== CONFIG ======\n",
        "# IMPORTANT: FILL THESE WITH YOUR REAL MILVUS DETAILS\n",
        "# You can find these in your Zilliz Cloud console or Milvus standalone setup.\n",
        "MILVUS_URI   = \"tx\"      # e.g. \"https://xxx.api.gcp-us-west1.zillizcloud.com\"\n",
        "MILVUS_TOKEN = \"ty\"    # Your API key or token\n",
        "\n",
        "RAI_COLLECTION    = \"rai_docs_v1\"\n",
        "VALUES_COLLECTION = \"values_hq\"\n",
        "\n",
        "EMB_MODEL_NAME    = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-dim\n",
        "\n",
        "# Hybrid classifier thresholds\n",
        "ZS_ACCEPT_THRESHOLD = 0.60   # zero-shot minimum score\n",
        "REQ_POS_KW = 1               # at least N positive keywords\n",
        "MAX_NEG_KW = 2               # at most N negative keywords\n",
        "\n",
        "# Text chunking\n",
        "MAX_TEXT_CHARS   = 4000\n",
        "CHUNK_SIZE_CHARS = 1800\n",
        "CHUNK_OVERLAP    = 200\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "print(\"Config loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "YWmlalbBtrPD",
        "outputId": "e7a827c7-e42e-4f5f-ff79-8a03b605a87d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to Milvus. Server version: Zilliz Cloud Vector Database(Compatible with Milvus 2.6)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/activations_tf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtf_keras\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/__internal__/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_initialize_variables\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minitialize_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrack_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/applications/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvNeXtBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtf_keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvNeXtLarge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tf_keras/src/applications/convnext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.compat'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4007988495.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 2. Initialize embedding model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0memb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMB_MODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 3. Infer embedding dimension dynamically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_huggingface/embeddings/huggingface.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0msentence_transformers\u001b[0m  \u001b[0;31m# type: ignore[import]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             msg = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mexport_static_quantized_openvino_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0;31m from sentence_transformers.cross_encoder import (\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mCrossEncoderModelCardData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/cross_encoder/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mCrossEncoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel_card\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoderModelCardData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoderTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_onnx_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_openvino_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_mixin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFitMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_card\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoderModelCardData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_model_card\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m from sentence_transformers.cross_encoder.util import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/cross_encoder/fit_mixin.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_args\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEncoderTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoDuplicatesDataLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNoDuplicatesDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentenceLabelDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceLabelDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentenceEvaluator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mDenoisingAutoEncoderDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenoisingAutoEncoderDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mNoDuplicatesDataLoader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNoDuplicatesDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mParallelSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentenceLabelDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceLabelDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mSentencesDataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentencesDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/datasets/ParallelSentencesDataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_card\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformerModelCardData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_model_card\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRouter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/model_card.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautonotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainerCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCodeCarbonCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelcard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_markdown_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_callback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2345\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2347\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2345\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2346\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2315\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2345\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2347\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2345\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2346\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDefaultDataCollator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mactivations_tf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tf_activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_object_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/activations_tf.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;34m\"Your currently installed version of Keras is Keras 3, but this is not yet supported in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;34m\"Transformers. Please install the backwards-compatible tf-keras package with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "from pymilvus import connections, utility\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# 1. Connect to Zilliz / Milvus\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=MILVUS_URI,\n",
        "    token=MILVUS_TOKEN,\n",
        ")\n",
        "\n",
        "print(\"Connected to Milvus. Server version:\", utility.get_server_version())\n",
        "\n",
        "# 2. Initialize embedding model\n",
        "emb_model = HuggingFaceEmbeddings(model_name=EMB_MODEL_NAME)\n",
        "\n",
        "# 3. Infer embedding dimension dynamically\n",
        "test_vec = emb_model.embed_query(\"test sentence about responsible AI\")\n",
        "EMB_DIM = len(test_vec)\n",
        "\n",
        "print(\"Embedding dimension:\", EMB_DIM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HfDctMiuHpD"
      },
      "outputs": [],
      "source": [
        "!pip install tf-keras\n",
        "\n",
        "# 3.1 Embeddings model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=EMB_MODEL_NAME)\n",
        "print(\"Embeddings model loaded.\")\n",
        "\n",
        "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
        "    # LangChain wrapper returns a list of list[float]\n",
        "    return embeddings.embed_documents(texts)\n",
        "\n",
        "# 3.2 Zero-shot classifier (force PyTorch backend)\n",
        "zs_classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\",\n",
        "    framework=\"pt\"\n",
        ")\n",
        "print(\"Zero-shot classifier loaded.\")\n",
        "\n",
        "# 3.3 Keywords\n",
        "POS_KEYWORDS = {\n",
        "    \"responsible ai\",\"ai safety\",\"ai governance\",\"ai ethics\",\"trustworthy ai\",\n",
        "    \"fairness\",\"bias\",\"discrimination\",\"transparency\",\"explainability\",\"accountability\",\n",
        "    \"privacy\",\"gdpr\",\"oecd\",\"nist\",\"eu ai act\",\"iso\",\"robustness\",\"reliability\",\n",
        "    \"risk management\",\"harm\",\"incident\",\"audit\",\"human oversight\",\"evaluation\",\"benchmark\",\n",
        "    \"policy\",\"standard\",\"compliance\",\"governance\",\"safety\",\"safeguard\"\n",
        "}   # presence increases the likelihood of the paper being added to the DB\n",
        "\n",
        "NEG_KEYWORDS = {\n",
        "    \"resnet\",\"vgg\",\"yolo\",\"gan\",\"segmentation\",\"transformer architecture\",\n",
        "    \"image classification\",\"object detection\",\"sota accuracy\",\"bleu score\",\"perplexity\",\n",
        "    \"fpga\",\"asic\",\"throughput\",\"compression ratio\",\"quantization aware training\",\n",
        "    \"wavelet\",\"svm kernel\",\"k-means clustering\",\"neural architecture search\",\n",
        "    \"convolutional\",\"backpropagation tricks\"\n",
        "}   # presence decreases the likelihood of the paper being added to the DB\n",
        "\n",
        "def keyword_stats(text: str) -> Tuple[int, int]:\n",
        "    '''\n",
        "    Counts the number of pos / neg keywords in a document\n",
        "    '''\n",
        "    t = text.lower()\n",
        "    pos = sum(1 for k in POS_KEYWORDS if k in t)\n",
        "    neg = sum(1 for k in NEG_KEYWORDS if k in t)\n",
        "    return pos, neg\n",
        "\n",
        "ZS_LABELS = [\n",
        "    \"responsible AI\", \"AI safety\", \"AI governance\", \"AI ethics\",\n",
        "    \"fairness\", \"transparency\", \"accountability\", \"privacy\",\n",
        "    \"robustness\", \"reliability\", \"risk management\", \"safety evaluation\"\n",
        "]   # will be stored as metadata in the DB; classes / types of papers\n",
        "\n",
        "def zero_shot_score(title: str, abstract: str) -> float:\n",
        "    '''\n",
        "\n",
        "    '''\n",
        "    txt = (title or \"\") + \"\\n\" + (abstract or \"\")\n",
        "    txt = txt[:2000]  # avoid very long\n",
        "    res = zs_classifier(txt, candidate_labels=ZS_LABELS, multi_label=True)\n",
        "    return float(max(res[\"scores\"])) if \"scores\" in res else 0.0\n",
        "\n",
        "def is_relevant(title: str, abstract: str) -> Tuple[bool, Dict]:\n",
        "    zs_score = zero_shot_score(title, abstract)\n",
        "    pos, neg = keyword_stats((title or \"\") + \" \" + (abstract or \"\"))\n",
        "    decision = (zs_score >= ZS_ACCEPT_THRESHOLD) and (pos >= REQ_POS_KW) and (neg <= MAX_NEG_KW)\n",
        "    return decision, {\"zs\": zs_score, \"pos\": pos, \"neg\": neg}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os22tZhJuPbK"
      },
      "outputs": [],
      "source": [
        "ARXIV_QUERIES = [\n",
        "    'ti:\"responsible AI\" OR \"responsible artificial intelligence\"',\n",
        "    '\"AI governance\" OR \"AI safety\" OR \"AI ethics\" OR \"trustworthy AI\"',\n",
        "    '\"AI risk management\" OR \"AI standards\" OR \"harm mitigation\" OR \"algorithmic fairness\"'\n",
        "]\n",
        "\n",
        "def search_arxiv(max_results: int = 150):\n",
        "    # Add delay_seconds to arxiv.Client to prevent rate limiting\n",
        "    client = arxiv.Client(delay_seconds=20)\n",
        "    found = {}\n",
        "    for q in ARXIV_QUERIES:\n",
        "        search = arxiv.Search(query=q, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n",
        "        for r in client.results(search):\n",
        "            key = r.entry_id\n",
        "            if key not in found:\n",
        "                found[key] = r\n",
        "    return list(found.values())\n",
        "\n",
        "def fetch_pdf_text(pdf_url: str) -> str:\n",
        "    try:\n",
        "        resp = requests.get(pdf_url, timeout=60)\n",
        "        resp.raise_for_status()\n",
        "        reader = PdfReader(io.BytesIO(resp.content))\n",
        "        pages = [pg.extract_text() or \"\" for pg in reader.pages]\n",
        "        return \"\\n\".join(pages)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"PDF fetch/parse failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_for_record(rec) -> Tuple[str, str, int, str, str]:\n",
        "    title = rec.title or \"\"\n",
        "    abstract = rec.summary or \"\"\n",
        "    year = rec.published.year if rec.published else -1\n",
        "    url = rec.entry_id\n",
        "\n",
        "    pdf_url = getattr(rec, \"pdf_url\", None)\n",
        "    if not pdf_url and \"/abs/\" in rec.entry_id:\n",
        "        pdf_url = rec.entry_id.replace(\"/abs/\", \"/pdf/\") + \".pdf\"\n",
        "\n",
        "    full_text = fetch_pdf_text(pdf_url) if pdf_url else \"\"\n",
        "    return title, abstract, year, full_text, url\n",
        "\n",
        "def chunk_text(text: str) -> List[str]:\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(text):\n",
        "        chunk = text[i:i+CHUNK_SIZE_CHARS]\n",
        "        chunks.append(chunk[:MAX_TEXT_CHARS])\n",
        "        i += max(CHUNK_SIZE_CHARS - CHUNK_OVERLAP, 1)\n",
        "    return chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41-IXHowueY5"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import List\n",
        "from pymilvus import Collection\n",
        "import logging\n",
        "import uuid\n",
        "from tqdm import tqdm\n",
        "from pymilvus import connections\n",
        "\n",
        "RAI_COLLECTION = \"rai_docs_v1\"\n",
        "\n",
        "def upsert_rai_chunks(\n",
        "    col: Collection,\n",
        "    doc_id: str,\n",
        "    title: str,\n",
        "    year: int,\n",
        "    url: str,\n",
        "    chunks: List[str],\n",
        "    embeddings: List[List[float]]\n",
        "):\n",
        "    n = len(chunks)\n",
        "    if n == 0:\n",
        "        return\n",
        "    data = [\n",
        "        [doc_id] * n,\n",
        "        [title] * n,\n",
        "        [int(year)] * n,\n",
        "        [\"arxiv\"] * n,\n",
        "        [url] * n,\n",
        "        [\"general\"] * n,\n",
        "        [\"unspecified\"] * n,\n",
        "        [\"unspecified\"] * n,\n",
        "        list(range(n)),\n",
        "        chunks,\n",
        "        embeddings\n",
        "    ]\n",
        "    col.insert(data)\n",
        "    col.flush()\n",
        "\n",
        "def ingest_rai_from_arxiv(limit: int = 150):\n",
        "    # Ensure Milvus connection is active before proceeding\n",
        "    if not connections.has_connection(\"default\"):\n",
        "        logging.info(\"Milvus connection not found, re-establishing...\")\n",
        "        connections.connect(alias=\"default\", uri=MILVUS_URI, token=MILVUS_TOKEN)\n",
        "        logging.info(\"Milvus connection re-established.\")\n",
        "\n",
        "    col = Collection(RAI_COLLECTION)\n",
        "    col.load()\n",
        "    records = search_arxiv(max_results=limit)\n",
        "    kept, skipped = 0, 0\n",
        "\n",
        "    for rec in tqdm(records, desc=\"Processing arXiv\"):\n",
        "        entry_id = rec.entry_id\n",
        "        title, abstract, year, full_text, url = extract_text_for_record(rec)\n",
        "\n",
        "        ok, stats = is_relevant(title, abstract)\n",
        "        if not ok:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        base_text = full_text if len(full_text) > 800 else (abstract or title)\n",
        "        chunks = chunk_text(base_text)\n",
        "        if not chunks:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        vecs = embed_texts(chunks)\n",
        "        doc_id = str(uuid.uuid5(uuid.NAMESPACE_URL, entry_id))\n",
        "        upsert_rai_chunks(col, doc_id, title, year, url, chunks, vecs)\n",
        "        kept += 1\n",
        "\n",
        "    print(f\"Ingest complete. Kept {kept} high-quality RAI/AI-safety papers; skipped {skipped} as irrelevant.\")\n",
        "\n",
        "MAX_ATTEMPTS = 3\n",
        "for attempt in range(MAX_ATTEMPTS):\n",
        "    try:\n",
        "        ingest_rai_from_arxiv(limit=30)\n",
        "        print(f\"Ingestion successful after {attempt+1} attempts.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        if \"HTTP 429\" in str(e) and attempt < MAX_ATTEMPTS - 1:\n",
        "            logging.warning(f\"Rate limit hit (attempt {attempt+1}/{MAX_ATTEMPTS}). Retrying in 60 seconds...\")\n",
        "            time.sleep(60)\n",
        "        else:\n",
        "            logging.error(f\"Ingestion failed after {attempt+1} attempts: {e}\")\n",
        "            raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcyMy8smuhlT"
      },
      "outputs": [],
      "source": [
        "CURATED_VALUES = [\n",
        "    {\n",
        "        \"value_name\": \"Fairness\",\n",
        "        \"description\": \"Avoid unjust bias and discrimination; design, measure, and mitigate disparate impact across groups when using AI models.\"\n",
        "    },\n",
        "    {\n",
        "        \"value_name\": \"Reliability\",\n",
        "        \"description\": \"Ensure consistent, stable, and robust AI behavior under expected conditions; implement monitoring, stress tests, and safe fallbacks.\"\n",
        "    },\n",
        "    {\n",
        "        \"value_name\": \"Transparency\",\n",
        "        \"description\": \"Make system capabilities and limitations visible to users and stakeholders; document data sources, model behavior, and decision boundaries.\"\n",
        "    },\n",
        "    {\n",
        "        \"value_name\": \"Explainability\",\n",
        "        \"description\": \"Provide meaningful, understandable explanations for predictions and decisions appropriate to the domain and user needs.\"\n",
        "    },\n",
        "    {\n",
        "        \"value_name\": \"Accountability\",\n",
        "        \"description\": \"Assign responsibility for AI outcomes; maintain audit trails; enable oversight and redress when harm or errors occur.\"\n",
        "    },\n",
        "    {\n",
        "        \"value_name\": \"Privacy\",\n",
        "        \"description\": \"Protect personal data in AI workflows using minimization, security controls, and lawful processing; reduce risk of data leakage.\"\n",
        "    },\n",
        "    {\n",
        "        \"value_name\": \"Security\",\n",
        "        \"description\": \"Safeguard AI systems and underlying data from attacks, tampering, and misuse through secure engineering practices.\"\n",
        "    },\n",
        "    {\n",
        "        \"value_name\": \"Human Oversight\",\n",
        "        \"description\": \"Design meaningful human control and escalation paths for high-risk decisions; ensure humans can intervene and override AI outputs.\"\n",
        "    },\n",
        "    {\n",
        "        \"value_name\": \"Safety\",\n",
        "        \"description\": \"Prevent harmful, dangerous, or destabilizing AI behaviors; conduct safety evaluations, red-teaming, and implement guardrails.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def ingest_values():\n",
        "    col = Collection(VALUES_COLLECTION)\n",
        "    col.load()\n",
        "    names = [v[\"value_name\"] for v in CURATED_VALUES]\n",
        "    descs = [v[\"description\"] for v in CURATED_VALUES]\n",
        "    text_for_emb = [f\"{n}: {d}\" for n, d in zip(names, descs)]\n",
        "    vecs = embed_texts(text_for_emb)\n",
        "    data = [\n",
        "        [0] * len(names), # Placeholder for 'id' field (auto_id=True)\n",
        "        names,\n",
        "        descs,\n",
        "        vecs\n",
        "    ]\n",
        "    col.insert(data)\n",
        "    col.flush()\n",
        "    print(f\"Ingested {len(CURATED_VALUES)} curated values into {VALUES_COLLECTION}.\")\n",
        "\n",
        "ingest_values()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Beez1OmHFlab"
      },
      "outputs": [],
      "source": [
        "import textwrap # Import textwrap for shortening text\n",
        "\n",
        "def search_rai(query: str, k: int = 5):\n",
        "    col = Collection(RAI_COLLECTION)\n",
        "    col.load()\n",
        "    # Use the globally defined embed_texts function from cell 8HfDctMiuHpD\n",
        "    q_vec = embed_texts([query])[0]\n",
        "    res = col.search(\n",
        "        data=[q_vec],\n",
        "        anns_field=\"embedding\",\n",
        "        param={\"metric_type\": \"IP\", \"params\": {\"ef\": 128}},\n",
        "        limit=k,\n",
        "        output_fields=[\"title\", \"year\", \"url\", \"text\", \"chunk_index\"]\n",
        "    )\n",
        "\n",
        "    hits = res[0]\n",
        "    for hit in hits:\n",
        "        # Construct dictionary from attributes of hit.entity\n",
        "        ent = {\n",
        "            'title': hit.entity.title if hasattr(hit.entity, 'title') else 'N/A',\n",
        "            'year': hit.entity.year if hasattr(hit.entity, 'year') else -1,\n",
        "            'url': hit.entity.url if hasattr(hit.entity, 'url') else 'N/A',\n",
        "            'text': hit.entity.text if hasattr(hit.entity, 'text') else 'N/A',\n",
        "            'chunk_index': hit.entity.chunk_index if hasattr(hit.entity, 'chunk_index') else -1\n",
        "        }\n",
        "        print(f\"[score={hit.distance:.3f}] {ent.get('title')} ({ent.get('year')})\")\n",
        "        print(f\"  url: {ent.get('url')}\")\n",
        "        print(\"  snippet:\", textwrap.shorten(ent.get(\"text\", \"\"), width=160))\n",
        "        print()\n",
        "    return hits\n",
        "\n",
        "def search_values(query: str, k: int = 5):\n",
        "    col = Collection(VALUES_COLLECTION)\n",
        "    col.load()\n",
        "    # Use the globally defined embed_texts function from cell 8HfDctMiuHpD\n",
        "    q_vec = embed_texts([query])[0]\n",
        "    res = col.search(\n",
        "        data=[q_vec],\n",
        "        anns_field=\"embedding\",\n",
        "        param={\"metric_type\": \"IP\", \"params\": {\"ef\": 128}},\n",
        "        limit=k,\n",
        "        output_fields=[\"value_name\", \"description\"]\n",
        "    )\n",
        "    for hit in res[0]:\n",
        "        # Construct dictionary from attributes of hit.entity\n",
        "        ent = {\n",
        "            'value_name': hit.entity.value_name if hasattr(hit.entity, 'value_name') else 'N/A',\n",
        "            'description': hit.entity.description if hasattr(hit.entity, 'description') else 'N/A'\n",
        "        }\n",
        "        print(f\"[score={hit.distance:.3f}] {ent.get('value_name')}\")\n",
        "        print(\"  \", textwrap.shorten(ent.get(\"description\", \"\"), width=160))\n",
        "        print()\n",
        "    return res[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "014hvqaVFx7H"
      },
      "outputs": [],
      "source": [
        "VALUE_REQUIREMENTS_MAP = {\n",
        "    \"Fairness\": [\n",
        "        \"Run demographic parity and disparate impact analysis.\",\n",
        "        \"Audit training data for representational bias.\",\n",
        "        \"Implement mitigation strategies for identified bias.\",\n",
        "        \"Document fairness risks and remediation steps.\",\n",
        "        \"Provide explainability for groups affected by decisions.\"\n",
        "    ],\n",
        "    \"Reliability\": [\n",
        "        \"Conduct stress tests under expected and unexpected loads.\",\n",
        "        \"Implement monitoring of system uptime and error rates.\",\n",
        "        \"Use fallback responses when the model is uncertain.\",\n",
        "        \"Design robust data validation and anomaly detection.\",\n",
        "        \"Track system performance drift over time.\"\n",
        "    ],\n",
        "    \"Transparency\": [\n",
        "        \"Provide model cards and data sheets.\",\n",
        "        \"Expose system capabilities and limitations to users.\",\n",
        "        \"Document training data sources and assumptions.\",\n",
        "        \"Maintain clear logs of decisions and model behavior.\"\n",
        "    ],\n",
        "    \"Privacy\": [\n",
        "        \"Minimize personal data collection.\",\n",
        "        \"Apply anonymization or differential privacy where possible.\",\n",
        "        \"Ensure encrypted data storage and transfer.\",\n",
        "        \"Provide users access, deletion, and data rights mechanisms.\"\n",
        "    ],\n",
        "    \"Security\": [\n",
        "        \"Implement adversarial robustness testing.\",\n",
        "        \"Protect model endpoints from injection attacks.\",\n",
        "        \"Secure training pipeline and data sources.\",\n",
        "        \"Monitor for misuse and anomalous access.\"\n",
        "    ],\n",
        "    \"Human Oversight\": [\n",
        "        \"Maintain a human-in-the-loop for high-impact decisions.\",\n",
        "        \"Allow escalation to a human supervisor.\",\n",
        "        \"Enable override mechanisms for incorrect predictions.\",\n",
        "        \"Define clear human accountability during deployment.\"\n",
        "    ],\n",
        "    \"Safety\": [\n",
        "        \"Conduct red-teaming and safety evaluations.\",\n",
        "        \"Identify potential harms and unintended consequences.\",\n",
        "        \"Implement guardrails and response filtering.\",\n",
        "        \"Monitor for emergent harmful behaviors.\",\n",
        "        \"Implement shutdown or safe-mode fallback mechanisms.\"\n",
        "    ],\n",
        "    \"Accountability\": [\n",
        "        \"Assign responsibility for AI outputs.\",\n",
        "        \"Maintain audit trails for important decisions.\",\n",
        "        \"Define redress paths for user grievances.\",\n",
        "        \"Document model lineage and versioning for traceability.\"\n",
        "    ],\n",
        "    \"Inclusiveness & Accessibility\": [\n",
        "        \"Ensure system usability for diverse user groups, including persons with disabilities.\",\n",
        "        \"Test system outputs for linguistic, cultural, and contextual inclusion.\",\n",
        "        \"Provide alternative interaction formats for users with access challenges.\",\n",
        "        \"Avoid exclusionary data assumptions that reduce model applicability.\"\n",
        "    ],\n",
        "    \"Environmental Sustainability\": [\n",
        "        \"Document energy consumption during model training and deployment.\",\n",
        "        \"Prefer efficient models when performance trade-offs are acceptable.\",\n",
        "        \"Monitor carbon footprint of large-scale inference workloads.\",\n",
        "        \"Use infrastructure providers with renewable-energy commitments.\"\n",
        "    ],\n",
        "    \"Explainability\": [\n",
        "        \"Provide user-friendly explanation interfaces.\",\n",
        "        \"Ensure explanations match the technical level of the audience.\",\n",
        "        \"Trace predictions to influential features or evidence.\",\n",
        "        \"Evaluate explanation consistency across similar cases.\"\n",
        "    ],\n",
        "    \"Integrity & Misuse Prevention\": [\n",
        "        \"Implement safeguards against manipulation and disinformation.\",\n",
        "        \"Restrict model capabilities that enable fraud, impersonation, or deception.\",\n",
        "        \"Monitor for misuse patterns and unauthorized system behavior.\",\n",
        "        \"Apply watermarking or provenance signals where applicable.\"\n",
        "    ],\n",
        "    \"Well-Being & Social Benefit\": [\n",
        "        \"Assess the societal impact of the system's deployment.\",\n",
        "        \"Ensure outputs do not cause psychological, economic, or social harm.\",\n",
        "        \"Embed constraints that prioritize user welfare in decision-making.\",\n",
        "        \"Evaluate long-term externalities of system behavior.\"\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHsXt87jIamc"
      },
      "outputs": [],
      "source": [
        "def get_requirements_for_values(selected_values):\n",
        "    results = {}\n",
        "    for v in selected_values:\n",
        "        if v in VALUE_REQUIREMENTS_MAP:\n",
        "            results[v] = VALUE_REQUIREMENTS_MAP[v]\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFu4HsjeIdbR"
      },
      "outputs": [],
      "source": [
        "USE_CASE_KEYWORDS = {\n",
        "    \"finance\": [\"loan\", \"credit\", \"bank\", \"risk score\", \"approval\"],\n",
        "    \"health\": [\"diagnosis\", \"medical\", \"triage\", \"therapy\", \"symptom\"],\n",
        "    \"hiring\": [\"recruitment\", \"cv\", \"resume\", \"job\", \"candidate\"],\n",
        "    \"education\": [\"exam\", \"grading\", \"assignment\", \"student\"],\n",
        "    \"surveillance\": [\"face recognition\", \"biometric\", \"tracking\"],\n",
        "    \"legal\": [\"court\", \"legal advice\", \"evidence\"],\n",
        "    \"general\": []\n",
        "}\n",
        "\n",
        "EU_HIGHRISK_MAP = {\n",
        "    \"finance\": True,\n",
        "    \"health\": True,\n",
        "    \"hiring\": True,\n",
        "    \"surveillance\": True,\n",
        "    \"education\": True,\n",
        "    \"legal\": True,\n",
        "    \"general\": False\n",
        "}\n",
        "\n",
        "def classify_use_case(description: str):\n",
        "    text = description.lower()\n",
        "    for domain, kws in USE_CASE_KEYWORDS.items():\n",
        "        for kw in kws:\n",
        "            if kw in text:\n",
        "                return domain, EU_HIGHRISK_MAP[domain]\n",
        "    return \"general\", False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01NHCU4zIgB-"
      },
      "outputs": [],
      "source": [
        "def merge_profile(values, use_case_description):\n",
        "    # 1. Classification\n",
        "    domain, highrisk = classify_use_case(use_case_description)\n",
        "\n",
        "    # 2. Requirements from selected values\n",
        "    value_reqs = get_requirements_for_values(values)\n",
        "\n",
        "    # 3. Retrieve insights from RAI DB\n",
        "    # search_rai prints results, so we capture nothing here\n",
        "    print(\"\\n--- Relevant RAI Insights (DB Search) ---\")\n",
        "    search_rai(use_case_description, k=5)\n",
        "\n",
        "    # 4. Return structured profile\n",
        "    return {\n",
        "        \"domain\": domain,\n",
        "        \"is_high_risk\": highrisk,\n",
        "        \"value_requirements\": value_reqs,\n",
        "        \"recommended_guardrails\": [\n",
        "            \"Implement human oversight for critical decisions.\",\n",
        "            \"Perform safety monitoring & drift detection.\",\n",
        "            \"Document decisions and preserve audit trails.\",\n",
        "            \"Apply privacy + security controls consistently.\"\n",
        "        ],\n",
        "        \"next_steps\": [\n",
        "            \"Complete risk assessment documentation.\",\n",
        "            \"Run fairness + robustness + privacy evaluations.\",\n",
        "            \"Prepare guardrails for deployment.\",\n",
        "            \"Map values to operational controls.\"\n",
        "        ]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6Xrq6aAIjG8"
      },
      "outputs": [],
      "source": [
        "def generate_report(profile):\n",
        "    print(\"\\n===== RESPONSIBLE AI REPORT =====\\n\")\n",
        "\n",
        "    print(f\"Domain: {profile['domain']}\")\n",
        "    print(f\"High-Risk (EU AI Act): {profile['is_high_risk']}\")\n",
        "\n",
        "    print(\"\\n--- Value Requirements ---\")\n",
        "    for v, reqs in profile[\"value_requirements\"].items():\n",
        "        print(f\"\\n{v}:\")\n",
        "        for r in reqs:\n",
        "            print(f\"  - {r}\")\n",
        "\n",
        "    print(\"\\n--- Recommended Guardrails ---\")\n",
        "    for g in profile[\"recommended_guardrails\"]:\n",
        "        print(f\" - {g}\")\n",
        "\n",
        "    print(\"\\n--- Next Steps ---\")\n",
        "    for s in profile[\"next_steps\"]:\n",
        "        print(f\" - {s}\")\n",
        "\n",
        "    print(\"\\n(Additional insights shown above in RAI DB search output.)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xak5zaMWN51g"
      },
      "outputs": [],
      "source": [
        "# The project uses HuggingFaceEmbeddings via embed_texts, making SentenceTransformer and embed_text redundant.\n",
        "# This cell is cleared to avoid redundancy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFV1OrcROfg4"
      },
      "outputs": [],
      "source": [
        "# This search_rai function is now defined in cell Beez1OmHFlab and uses embed_texts.\n",
        "# This cell is cleared to avoid redundancy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guZJM6DvOiC2"
      },
      "outputs": [],
      "source": [
        "def merge_profile(values, use_case_description):\n",
        "    # 1. Classification\n",
        "    domain, highrisk = classify_use_case(use_case_description)\n",
        "\n",
        "    # 2. Requirements from selected values\n",
        "    value_reqs = get_requirements_for_values(values)\n",
        "\n",
        "    # 3. Retrieve insights from RAI DB\n",
        "    print(\"\\n--- Relevant RAI Insights (DB Search) ---\")\n",
        "    search_rai(use_case_description, k=5)\n",
        "\n",
        "    # 4. Return structured profile\n",
        "    return {\n",
        "        \"domain\": domain,\n",
        "        \"is_high_risk\": highrisk,\n",
        "        \"value_requirements\": value_reqs,\n",
        "        \"recommended_guardrails\": [\n",
        "            \"Implement human oversight for critical decisions.\",\n",
        "            \"Perform safety monitoring & drift detection.\",\n",
        "            \"Document decisions and preserve audit trails.\",\n",
        "            \"Apply privacy + security controls consistently.\"\n",
        "        ],\n",
        "        \"next_steps\": [\n",
        "            \"Complete risk assessment documentation.\",\n",
        "            \"Run fairness + robustness + privacy evaluations.\",\n",
        "            \"Prepare guardrails for deployment.\",\n",
        "            \"Map values to operational controls.\"\n",
        "        ]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "504d0fb2"
      },
      "outputs": [],
      "source": [
        "profile = merge_profile(\n",
        "    values=[\"Fairness\", \"Reliability\", \"Privacy\"],\n",
        "    use_case_description=\"We are building a loan approval chatbot for consumer microfinance.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65ae4a6b"
      },
      "outputs": [],
      "source": [
        "generate_report(profile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQitoCu1R8Ph"
      },
      "outputs": [],
      "source": [
        "profile = merge_profile(\n",
        "    values=[\"Fairness\"],\n",
        "    use_case_description=\"We are building AI for screening job applicants.\"\n",
        ")\n",
        "generate_report(profile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20dIV8Xy_dMl"
      },
      "outputs": [],
      "source": [
        "profile = merge_profile(\n",
        "    values=[\"Safety\", \"Human Oversight\"],\n",
        "    use_case_description=\"We are creating an AI system to assist in medical diagnosis.\"\n",
        ")\n",
        "generate_report(profile)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "profile = merge_profile(\n",
        "    values=[\"Well-Being\"],\n",
        "    use_case_description=\"We are creating an AI system for ensuring mental health wellness.\"\n",
        ")\n",
        "generate_report(profile)"
      ],
      "metadata": {
        "id": "dj-1FaL2rzo3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}