{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHiVUaSJ6UAO",
        "outputId": "62f2db14-e908-46d5-e415-177664175413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/273.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m266.2/273.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pymilvus sentence-transformers pandas openpyxl --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import connections\n",
        "\n",
        "MILVUS_URI = \"https://in03-001d3ad9002d3cc.serverless.aws-eu-central-1.cloud.zilliz.com\"\n",
        "MILVUS_TOKEN = \"ffce38880bf73e2a6abcf7364940ec0a13d8d584639f5f03145fe73723ad0080c72f814e6759b346998208a9fbea885b44848673\"\n",
        "\n",
        "connections.connect(\n",
        "    alias=\"default\",\n",
        "    uri=MILVUS_URI,\n",
        "    token=MILVUS_TOKEN,\n",
        ")\n",
        "\n",
        "print(\"Connected:\", connections.has_connection(\"default\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw8SI3wF7AvW",
        "outputId": "11c2ffbf-54b3-420f-cbc3-ce75844fbfcc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"/content/RAI_Papers_Expanded_HQ.xlsx\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "HwOHHrYi8Zav",
        "outputId": "af67cbca-22f2-4c8c-f58e-af071e928453"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/RAI_Papers_Expanded_HQ.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2725507295.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/RAI_Papers_Expanded_HQ.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/RAI_Papers_Expanded_HQ.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "EMBED_DIM = embedding_model.get_sentence_embedding_dimension()\n",
        "\n",
        "print(\"Embedding dimension:\", EMBED_DIM)\n"
      ],
      "metadata": {
        "id": "lbRMvi7i8fUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import (\n",
        "    FieldSchema, CollectionSchema, DataType, Collection, utility\n",
        ")\n",
        "\n",
        "COLLECTION_NAME = \"rai_papers_hq\"\n",
        "\n",
        "def create_collection():\n",
        "    if utility.has_collection(COLLECTION_NAME):\n",
        "        print(f\"Collection `{COLLECTION_NAME}` already exists.\")\n",
        "        return Collection(COLLECTION_NAME)\n",
        "\n",
        "    fields = [\n",
        "        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
        "        FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=512),\n",
        "        FieldSchema(name=\"domain\", dtype=DataType.VARCHAR, max_length=128),\n",
        "        FieldSchema(name=\"risk_area\", dtype=DataType.VARCHAR, max_length=128),\n",
        "        FieldSchema(name=\"lifecycle_phase\", dtype=DataType.VARCHAR, max_length=128),\n",
        "        FieldSchema(name=\"year\", dtype=DataType.INT64),\n",
        "        FieldSchema(name=\"link\", dtype=DataType.VARCHAR, max_length=512),\n",
        "        FieldSchema(name=\"abstract\", dtype=DataType.VARCHAR, max_length=4000),\n",
        "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBED_DIM),\n",
        "    ]\n",
        "\n",
        "    schema = CollectionSchema(fields, description=\"Responsible AI / AI Safety Papers\")\n",
        "\n",
        "    collection = Collection(\n",
        "        name=COLLECTION_NAME,\n",
        "        schema=schema,\n",
        "        using=\"default\",\n",
        "        shards_num=2,\n",
        "    )\n",
        "\n",
        "    index_params = {\n",
        "        \"index_type\": \"HNSW\",\n",
        "        \"metric_type\": \"COSINE\",\n",
        "        \"params\": {\"M\": 16, \"efConstruction\": 200},\n",
        "    }\n",
        "    collection.create_index(\"embedding\", index_params)\n",
        "\n",
        "    print(f\"Collection `{COLLECTION_NAME}` created and indexed.\")\n",
        "    return collection\n",
        "\n",
        "collection = create_collection()\n"
      ],
      "metadata": {
        "id": "x2zhWDh_9gAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "df[\"id\"] = df[\"id\"].astype(int)\n",
        "\n",
        "def get_text(row):\n",
        "    txt = str(row.get(\"abstract\") or \"\").strip()\n",
        "    if not txt:\n",
        "        txt = str(row.get(\"title\") or \"\")\n",
        "    return txt\n",
        "\n",
        "def embed_and_insert(df, collection, batch_size=32):\n",
        "    ids = []\n",
        "    titles = []\n",
        "    domains = []\n",
        "    risks = []\n",
        "    phases = []\n",
        "    years = []\n",
        "    links = []\n",
        "    abstracts = []\n",
        "    embeddings = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        text = get_text(row)\n",
        "        vec = embedding_model.encode(text)\n",
        "\n",
        "        ids.append(int(row[\"id\"]))\n",
        "        titles.append(str(row[\"title\"]))\n",
        "        domains.append(str(row[\"domain\"]))\n",
        "        risks.append(str(row[\"risk_area\"]))\n",
        "        phases.append(str(row[\"lifecycle_phase\"]))\n",
        "        years.append(int(row[\"year\"]))\n",
        "        links.append(str(row[\"link\"]))\n",
        "        abstracts.append(text)\n",
        "        embeddings.append(vec)\n",
        "\n",
        "        if len(ids) >= batch_size:\n",
        "            collection.insert([\n",
        "                ids, titles, domains, risks, phases, years, links, abstracts, embeddings\n",
        "            ])\n",
        "            ids, titles, domains, risks, phases, years, links, abstracts, embeddings = [], [], [], [], [], [], [], [], []\n",
        "\n",
        "    # last batch\n",
        "    if ids:\n",
        "        collection.insert([\n",
        "            ids, titles, domains, risks, phases, years, links, abstracts, embeddings\n",
        "        ])\n",
        "\n",
        "    collection.flush()\n",
        "    print(\"Done inserting rows.\")\n",
        "\n",
        "embed_and_insert(df, collection)\n"
      ],
      "metadata": {
        "id": "GmgbfVpC9lca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_filter_expr(domain=None, risk_area=None, lifecycle_phase=None, min_year=None, max_year=None):\n",
        "    expr = []\n",
        "    if domain:\n",
        "        expr.append(f'domain == \"{domain}\"')\n",
        "    if risk_area:\n",
        "        expr.append(f'risk_area == \"{risk_area}\"')\n",
        "    if lifecycle_phase:\n",
        "        expr.append(f'lifecycle_phase == \"{lifecycle_phase}\"')\n",
        "    if min_year is not None:\n",
        "        expr.append(f'year >= {min_year}')\n",
        "    if max_year is not None:\n",
        "        expr.append(f'year <= {max_year}')\n",
        "    return \" and \".join(expr) if expr else None\n",
        "\n",
        "\n",
        "def search_rai(query, top_k=5, domain=None, risk_area=None, lifecycle_phase=None, min_year=None, max_year=None):\n",
        "    query_vec = embedding_model.encode(query)\n",
        "\n",
        "    expr = build_filter_expr(domain, risk_area, lifecycle_phase, min_year, max_year)\n",
        "    print(\"Filter:\", expr)\n",
        "\n",
        "    # Load the collection into memory before searching\n",
        "    collection.load()\n",
        "\n",
        "    results = collection.search(\n",
        "        data=[query_vec],\n",
        "        anns_field=\"embedding\",\n",
        "        param={\"metric_type\": \"COSINE\", \"params\": {\"ef\": 128}},\n",
        "        limit=top_k,\n",
        "        expr=expr,\n",
        "        output_fields=[\"title\", \"domain\", \"risk_area\", \"lifecycle_phase\", \"year\", \"link\", \"abstract\"],\n",
        "        consistency_level=\"Strong\"\n",
        "    )\n",
        "    return results[0]"
      ],
      "metadata": {
        "id": "AyhstkGW9tXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hits = search_rai(\n",
        "    \"requirements for high-risk AI systems\",\n",
        "    domain=\"governance\"\n",
        ")\n",
        "\n",
        "for h in hits:\n",
        "    print(\"→\", h.entity.get(\"title\"), \"|\", h.entity.get(\"year\"))\n"
      ],
      "metadata": {
        "id": "X-uWNKjJ9w_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import utility, Collection\n",
        "\n",
        "print(\"Has connection:\", connections.has_connection(\"default\"))\n",
        "print(\"Has collection rai_papers_hq:\", utility.has_collection(\"rai_papers_hq\"))\n",
        "\n",
        "collection = Collection(\"rai_papers_hq\")\n",
        "collection.load()\n",
        "print(\"Number of entities:\", collection.num_entities)"
      ],
      "metadata": {
        "id": "tMxEv_q9Bv2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# (Re-use your existing embedding_model; only run this if it's missing)\n",
        "# embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "query_vec = embedding_model.encode(\"test query about responsible AI\")\n",
        "\n",
        "results = collection.search(\n",
        "    data=[query_vec],\n",
        "    anns_field=\"embedding\",\n",
        "    param={\"metric_type\": \"COSINE\", \"params\": {\"ef\": 128}},\n",
        "    limit=3,\n",
        "    output_fields=[\"title\", \"year\", \"link\", \"domain\", \"risk_area\"]\n",
        ")\n",
        "\n",
        "for hit in results[0]:\n",
        "    print(\"Score (distance):\", hit.distance)\n",
        "    print(\"Title:\", hit.entity.get(\"title\"))\n",
        "    print(\"Year:\", hit.entity.get(\"year\"))\n",
        "    print(\"Domain:\", hit.entity.get(\"domain\"))\n",
        "    print(\"Risk area:\", hit.entity.get(\"risk_area\"))\n",
        "    print(\"Link:\", hit.entity.get(\"link\"))\n",
        "    print(\"----\")\n"
      ],
      "metadata": {
        "id": "Q3IEWZ2pCARR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_filter_expr(domain=None, risk_area=None, lifecycle_phase=None, min_year=None, max_year=None):\n",
        "    expr_parts = []\n",
        "    if domain:\n",
        "        expr_parts.append(f'domain == \"{domain}\"')\n",
        "    if risk_area:\n",
        "        expr_parts.append(f'risk_area == \"{risk_area}\"')\n",
        "    if lifecycle_phase:\n",
        "        expr_parts.append(f'lifecycle_phase == \"{lifecycle_phase}\"')\n",
        "    if min_year is not None:\n",
        "        expr_parts.append(f'year >= {int(min_year)}')\n",
        "    if max_year is not None:\n",
        "        expr_parts.append(f'year <= {int(max_year)}')\n",
        "    if not expr_parts:\n",
        "        return None\n",
        "    return \" and \".join(expr_parts)\n"
      ],
      "metadata": {
        "id": "Eh4Lh3YpCdC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_rai(\n",
        "    query,\n",
        "    top_k=5,\n",
        "    domain=None,\n",
        "    risk_area=None,\n",
        "    lifecycle_phase=None,\n",
        "    min_year=None,\n",
        "    max_year=None,\n",
        "):\n",
        "    query_vec = embedding_model.encode(query)\n",
        "    expr = build_filter_expr(domain, risk_area, lifecycle_phase, min_year, max_year)\n",
        "    print(\"Filter:\", expr)\n",
        "\n",
        "    search_params = {\"metric_type\": \"COSINE\", \"params\": {\"ef\": 128}}\n",
        "\n",
        "    if expr:\n",
        "        results = collection.search(\n",
        "            data=[query_vec],\n",
        "            anns_field=\"embedding\",\n",
        "            param=search_params,\n",
        "            limit=top_k,\n",
        "            expr=expr,\n",
        "            output_fields=[\n",
        "                \"title\",\n",
        "                \"domain\",\n",
        "                \"risk_area\",\n",
        "                \"lifecycle_phase\",\n",
        "                \"year\",\n",
        "                \"link\",\n",
        "                \"abstract\",\n",
        "            ],\n",
        "        )\n",
        "    else:\n",
        "        results = collection.search(\n",
        "            data=[query_vec],\n",
        "            anns_field=\"embedding\",\n",
        "            param=search_params,\n",
        "            limit=top_k,\n",
        "            output_fields=[\n",
        "                \"title\",\n",
        "                \"domain\",\n",
        "                \"risk_area\",\n",
        "                \"lifecycle_phase\",\n",
        "                \"year\",\n",
        "                \"link\",\n",
        "                \"abstract\",\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    return results[0]\n"
      ],
      "metadata": {
        "id": "C6jgLbtQChQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hits = search_rai(\"requirements for high-risk AI systems\", top_k=5)\n",
        "\n",
        "for h in hits:\n",
        "    print(\"Title:\", h.entity.get(\"title\"))\n",
        "    print(\"Year:\", h.entity.get(\"year\"))\n",
        "    print(\"Domain:\", h.entity.get(\"domain\"))\n",
        "    print(\"Risk area:\", h.entity.get(\"risk_area\"))\n",
        "    print(\"Link:\", h.entity.get(\"link\"))\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "id": "l3R3gR33CjCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection, utility\n",
        "\n",
        "VALUES_COLLECTION_NAME = \"values_hq\"\n",
        "\n",
        "def create_values_collection():\n",
        "    if utility.has_collection(VALUES_COLLECTION_NAME):\n",
        "        print(f\"Collection `{VALUES_COLLECTION_NAME}` already exists.\")\n",
        "        return Collection(VALUES_COLLECTION_NAME)\n",
        "\n",
        "    fields = [\n",
        "        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
        "        FieldSchema(name=\"value_name\", dtype=DataType.VARCHAR, max_length=256),\n",
        "        FieldSchema(name=\"description\", dtype=DataType.VARCHAR, max_length=4000),\n",
        "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBED_DIM),\n",
        "    ]\n",
        "\n",
        "    schema = CollectionSchema(\n",
        "        fields=fields,\n",
        "        description=\"Stakeholder and Responsible AI values for alignment.\"\n",
        "    )\n",
        "\n",
        "    values_collection = Collection(\n",
        "        name=VALUES_COLLECTION_NAME,\n",
        "        schema=schema,\n",
        "        using=\"default\",\n",
        "        shards_num=2,\n",
        "    )\n",
        "\n",
        "    index_params = {\n",
        "        \"index_type\": \"HNSW\",\n",
        "        \"metric_type\": \"COSINE\",\n",
        "        \"params\": {\"M\": 16, \"efConstruction\": 200},\n",
        "    }\n",
        "    values_collection.create_index(\"embedding\", index_params)\n",
        "\n",
        "    print(f\"Collection `{VALUES_COLLECTION_NAME}` created and indexed.\")\n",
        "    return values_collection\n",
        "\n",
        "values_collection = create_values_collection()\n",
        "values_collection.load()\n"
      ],
      "metadata": {
        "id": "zLn-4vHBCrmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VALUES_DATA = [\n",
        "    # 1–10: Core fairness / transparency / accountability\n",
        "    {\"id\": 1, \"value_name\": \"Fairness\", \"description\": \"Avoid unjust bias, discrimination, or unfair treatment of individuals or groups in data, models, and decisions.\"},\n",
        "    {\"id\": 2, \"value_name\": \"Non-discrimination\", \"description\": \"Ensure the system does not treat people differently based on protected characteristics such as race, gender, religion, or disability.\"},\n",
        "    {\"id\": 3, \"value_name\": \"Equity\", \"description\": \"Design and evaluate systems to reduce unjust gaps in outcomes across different groups, not just treat everyone the same.\"},\n",
        "    {\"id\": 4, \"value_name\": \"Transparency\", \"description\": \"Make key aspects of the AI system visible and understandable to appropriate stakeholders, including limitations and risks.\"},\n",
        "    {\"id\": 5, \"value_name\": \"Accountability\", \"description\": \"Ensure that humans and organisations remain responsible for AI outcomes, with clear roles, escalation paths, and remediation processes.\"},\n",
        "    {\"id\": 6, \"value_name\": \"Explainability\", \"description\": \"Provide meaningful explanations of how inputs relate to outputs so that stakeholders can understand and challenge decisions.\"},\n",
        "    {\"id\": 7, \"value_name\": \"Interpretability\", \"description\": \"Use models and techniques that allow practitioners to inspect and reason about internal behaviour or feature influence.\"},\n",
        "    {\"id\": 8, \"value_name\": \"Traceability\", \"description\": \"Maintain links between data, models, decisions, and requirements so that behaviour can be audited and reconstructed.\"},\n",
        "    {\"id\": 9, \"value_name\": \"Auditability\", \"description\": \"Design systems so that internal processes and outcomes can be independently assessed for compliance and performance.\"},\n",
        "    {\"id\": 10, \"value_name\": \"Reproducibility\", \"description\": \"Ensure that model training and evaluation can be repeated with consistent results given the same data and configuration.\"},\n",
        "\n",
        "    # 11–20: Safety / robustness / reliability / security\n",
        "    {\"id\": 11, \"value_name\": \"Safety\", \"description\": \"Avoid causing physical, psychological, economic, or societal harm through AI behaviour, both in normal and stressed conditions.\"},\n",
        "    {\"id\": 12, \"value_name\": \"Robustness\", \"description\": \"Ensure the system behaves reliably under distribution shifts, noisy inputs, and adversarial attempts to break it.\"},\n",
        "    {\"id\": 13, \"value_name\": \"Reliability\", \"description\": \"Deliver consistent, predictable performance over time and across relevant scenarios.\"},\n",
        "    {\"id\": 14, \"value_name\": \"Security\", \"description\": \"Protect the system, data, and models from unauthorised access, tampering, extraction, or adversarial manipulation.\"},\n",
        "    {\"id\": 15, \"value_name\": \"Resilience\", \"description\": \"Design systems to recover gracefully from failures, attacks, or unexpected behaviour with minimal harm.\"},\n",
        "    {\"id\": 16, \"value_name\": \"Misuse Prevention\", \"description\": \"Reduce the likelihood that the system will be used to facilitate harmful, illegal, or unethical activities.\"},\n",
        "    {\"id\": 17, \"value_name\": \"Long-term Safety\", \"description\": \"Consider potential long-horizon impacts of deploying the system, including emergent risks and compounding effects.\"},\n",
        "    {\"id\": 18, \"value_name\": \"Safe Intervention\", \"description\": \"Allow human operators to understand when intervention is needed and safely pause, override, or shut down the system.\"},\n",
        "    {\"id\": 19, \"value_name\": \"Corrigibility\", \"description\": \"Design the system so it cooperates with human oversight and can be corrected without resisting or disabling that oversight.\"},\n",
        "    {\"id\": 20, \"value_name\": \"Predictability\", \"description\": \"Ensure that system behaviour can be anticipated within reasonable bounds by domain experts and operators.\"},\n",
        "\n",
        "    # 21–30: Human-centred / ethics / well-being / autonomy\n",
        "    {\"id\": 21, \"value_name\": \"Human Agency\", \"description\": \"Support human decision-making rather than replacing it inappropriately; preserve meaningful human control over important outcomes.\"},\n",
        "    {\"id\": 22, \"value_name\": \"Human Oversight\", \"description\": \"Embed mechanisms for humans to monitor, review, and override AI decisions where necessary.\"},\n",
        "    {\"id\": 23, \"value_name\": \"Respect for Autonomy\", \"description\": \"Avoid manipulating, coercing, or unduly nudging users; support informed and voluntary choices.\"},\n",
        "    {\"id\": 24, \"value_name\": \"Respect for Dignity\", \"description\": \"Treat individuals as ends in themselves, not merely as data points, and avoid demeaning or dehumanising uses of AI.\"},\n",
        "    {\"id\": 25, \"value_name\": \"Well-being\", \"description\": \"Promote the physical, mental, and social well-being of users and affected communities.\"},\n",
        "    {\"id\": 26, \"value_name\": \"Non-harm\", \"description\": \"Avoid causing unnecessary harm and minimise unavoidable harms through good design and safeguards.\"},\n",
        "    {\"id\": 27, \"value_name\": \"Beneficence\", \"description\": \"Actively seek to create positive benefits for users and society through the AI system.\"},\n",
        "    {\"id\": 28, \"value_name\": \"Justice\", \"description\": \"Distribute benefits and burdens of AI fairly across individuals and groups.\"},\n",
        "    {\"id\": 29, \"value_name\": \"Accessibility\", \"description\": \"Ensure the system is usable and understandable by people with diverse abilities, backgrounds, and contexts.\"},\n",
        "    {\"id\": 30, \"value_name\": \"Cultural Sensitivity\", \"description\": \"Design and deploy AI in a way that respects local norms, languages, and cultural contexts.\"},\n",
        "\n",
        "    # 31–40: Data governance / privacy / lifecycle / environment\n",
        "    {\"id\": 31, \"value_name\": \"Privacy\", \"description\": \"Respect individuals’ rights to control their personal data and limit intrusive surveillance or inference.\"},\n",
        "    {\"id\": 32, \"value_name\": \"Data Governance\", \"description\": \"Manage data collection, labelling, storage, access, and deletion according to policies and regulations.\"},\n",
        "    {\"id\": 33, \"value_name\": \"Data Quality\", \"description\": \"Use data that is as accurate, relevant, representative, and up to date as reasonably possible for the task.\"},\n",
        "    {\"id\": 34, \"value_name\": \"Data Minimisation\", \"description\": \"Collect and use only the data that is necessary to achieve legitimate purposes.\"},\n",
        "    {\"id\": 35, \"value_name\": \"Lifecycle Governance\", \"description\": \"Apply governance practices consistently from ideation through deployment and retirement of the system.\"},\n",
        "    {\"id\": 36, \"value_name\": \"Monitoring & Feedback\", \"description\": \"Continuously monitor system behaviour in production and integrate feedback to fix issues and improve alignment.\"},\n",
        "    {\"id\": 37, \"value_name\": \"Incident Reporting\", \"description\": \"Provide channels and processes for reporting, investigating, and resolving safety or ethics incidents.\"},\n",
        "    {\"id\": 38, \"value_name\": \"Sustainability\", \"description\": \"Consider environmental impacts of data, compute, and deployment and aim to minimise unnecessary resource use.\"},\n",
        "    {\"id\": 39, \"value_name\": \"Social Benefit\", \"description\": \"Align the system with broader societal goals, avoiding applications that primarily create harm or extraction.\"},\n",
        "    {\"id\": 40, \"value_name\": \"Legal Compliance\", \"description\": \"Ensure the system adheres to applicable laws, regulations, and standards in each deployment context.\"},\n",
        "\n",
        "    # 41–50: Alignment-specific / documentation / trust\n",
        "    {\"id\": 41, \"value_name\": \"Goal Alignment\", \"description\": \"Align the system’s optimisation objective with the human-defined objectives and constraints for the task and domain.\"},\n",
        "    {\"id\": 42, \"value_name\": \"Value Alignment\", \"description\": \"Align AI behaviour with the stated values and principles of the organisation and its stakeholders.\"},\n",
        "    {\"id\": 43, \"value_name\": \"Reward Design Integrity\", \"description\": \"Design reward signals and metrics that reflect true success, and avoid easy-to-game proxies that encourage bad behaviour.\"},\n",
        "    {\"id\": 44, \"value_name\": \"Misuse Resistance\", \"description\": \"Reduce the ability of users or attackers to repurpose the system for harmful, unethical, or illegal aims.\"},\n",
        "    {\"id\": 45, \"value_name\": \"Documentation\", \"description\": \"Produce clear, honest documentation of data, models, limitations, risks, and appropriate use cases.\"},\n",
        "    {\"id\": 46, \"value_name\": \"User Education\", \"description\": \"Help users understand how the system works, when it may fail, and how to use it responsibly.\"},\n",
        "    {\"id\": 47, \"value_name\": \"Trustworthiness\", \"description\": \"Earn justified trust from users and stakeholders by consistently demonstrating reliable, ethical, and transparent behaviour.\"},\n",
        "    {\"id\": 48, \"value_name\": \"Context-Appropriate Use\", \"description\": \"Deploy the system only in use cases and environments where its performance, risks, and safeguards are adequate.\"},\n",
        "    {\"id\": 49, \"value_name\": \"Continuous Improvement\", \"description\": \"Iteratively refine the system, values mapping, and safeguards as new risks, data, and standards emerge.\"},\n",
        "    {\"id\": 50, \"value_name\": \"Alignment with Stakeholder Values\", \"description\": \"Ensure that the system’s behaviour reflects the articulated values of affected stakeholders, not only the organisation’s internal priorities.\"},\n",
        "]\n"
      ],
      "metadata": {
        "id": "hHgYOJNYqlQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_values(values_data, values_collection):\n",
        "    ids = []\n",
        "    names = []\n",
        "    descriptions = []\n",
        "    embeddings = []\n",
        "\n",
        "    for row in values_data:\n",
        "        text_for_embedding = f\"{row['value_name']}: {row['description']}\"\n",
        "        vec = embedding_model.encode(text_for_embedding)\n",
        "\n",
        "        ids.append(int(row[\"id\"]))\n",
        "        names.append(row[\"value_name\"])\n",
        "        descriptions.append(row[\"description\"])\n",
        "        embeddings.append(vec)\n",
        "\n",
        "    values_collection.insert([\n",
        "        ids,\n",
        "        names,\n",
        "        descriptions,\n",
        "        embeddings,\n",
        "    ])\n",
        "    values_collection.flush()\n",
        "    print(f\"Ingested {len(ids)} values into `values_hq`.\")\n",
        "\n",
        "ingest_values(VALUES_DATA, values_collection)\n"
      ],
      "metadata": {
        "id": "skea7L4gqsuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_values(query, top_k=5):\n",
        "    values_collection = Collection(VALUES_COLLECTION_NAME)\n",
        "    values_collection.load()\n",
        "\n",
        "    q_vec = embedding_model.encode(query)\n",
        "    search_params = {\"metric_type\": \"COSINE\", \"params\": {\"ef\": 128}}\n",
        "\n",
        "    results = values_collection.search(\n",
        "        data=[q_vec],\n",
        "        anns_field=\"embedding\",\n",
        "        param=search_params,\n",
        "        limit=top_k,\n",
        "        output_fields=[\"value_name\", \"description\"],\n",
        "    )\n",
        "    return results[0]\n"
      ],
      "metadata": {
        "id": "LllT4O-mqyc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hits = search_values(\"avoid bias in credit decisions and treat groups fairly\", top_k=5)\n",
        "for h in hits:\n",
        "    print(h.entity.get(\"value_name\"), \"|\", h.entity.get(\"description\"))\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "id": "2-XDNZ1iq56W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hits = search_values(\"keep humans in control and able to override the AI\", top_k=5)\n",
        "for h in hits:\n",
        "    print(h.entity.get(\"value_name\"))\n"
      ],
      "metadata": {
        "id": "Df28qRyarANh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YbKnvjSA1_D0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}